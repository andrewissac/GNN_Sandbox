{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import copy\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from trainAndEval import train, evaluate\n",
    "from dgl.data import DGLDataset\n",
    "from TrainResults import TrainResults\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from SimpleGCN_OnlyNodeFeatures import GCN_OnlyNodeFeatures\n",
    "from ToyDGLDataset_v2 import ToyDGLDataset_v2, GetNodeFeatureVectors, GetEdgeFeatureVectors, GetNeighborNodes, GetEdgeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def trainEpochs(model, device, dataloader, optimizer, loss_fn, batchsize, nEpochs):\n",
    "    results = TrainResults()\n",
    "    bestModel = None\n",
    "    bestValAcc = 0.0\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "        # train\n",
    "        epochLoss = train(model, device, dataloader, optimizer, loss_fn, batchsize, results)\n",
    "\n",
    "        # evaluate\n",
    "        train_result = evaluate(model, device, train_dataloader)\n",
    "        val_result = evaluate(model, device, val_dataloader)\n",
    "        test_result = evaluate(model, device, test_dataloader)\n",
    "\n",
    "        results.addEpochResult(epochLoss, train_result, val_result, test_result)\n",
    "\n",
    "        if(epoch % 5 == 0):\n",
    "            results.printLastResult()\n",
    "\n",
    "        if results.best_val_acc > bestValAcc:\n",
    "            bestValAcc = results.best_val_acc\n",
    "            bestModel = copy.deepcopy(model)\n",
    "\n",
    "    return results, bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllDatasetNames(datasetRootDir):\n",
    "    files = glob.glob(datasetRootDir + '/*/*/*.json', recursive=True)\n",
    "    files = [x for x in files if \"Toy2_v3_0\" in x]\n",
    "    files.sort()\n",
    "    datasetDirectories = [path.dirname(file) for file in files]\n",
    "    datasetnames = [path.normpath(dir).split(path.sep)[-1] for dir in datasetDirectories]\n",
    "    return datasetDirectories, datasetnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "datasetRootDir = '/home/andrew/GNN_Sandbox/GraphToyDatasets_v3'\n",
    "datasetDirs, datasetNames = getAllDatasetNames(datasetRootDir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datasetName = datasetNames[0]\n",
    "datasetDir = datasetDirs[0]\n",
    "dataset = ToyDGLDataset_v2(datasetName, datasetDir)\n",
    "dataset.printProperties()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "graph, label = dataset[0]\n",
    "print(graph)\n",
    "print(f'Label: {label}')\n",
    "print(GetNodeFeatureVectors(graph))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batched_graph, labels = batch\n",
    "print('Number of nodes for each graph element in the batch:', batched_graph.batch_num_nodes())\n",
    "print('Number of edges for each graph element in the batch:', batched_graph.batch_num_edges())\n",
    "print(labels)\n",
    "# Recover the original graph elements from the minibatch\n",
    "graphs = dgl.unbatch(batched_graph)\n",
    "print('The original graphs in the minibatch:')\n",
    "print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_0percentDiff\n",
      "Epoch: 0, Loss: 77.8142, Train: 0.502, Valid: 0.501, Test: 0.492, AUC: 0.491\n",
      "Epoch: 5, Loss: 13.1317, Train: 0.498, Valid: 0.503, Test: 0.507, AUC: 0.508\n",
      "Epoch: 10, Loss: 4.8099, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.507\n",
      "Epoch: 15, Loss: 9.9701, Train: 0.498, Valid: 0.506, Test: 0.504, AUC: 0.508\n",
      "Epoch: 20, Loss: 11.8910, Train: 0.498, Valid: 0.502, Test: 0.506, AUC: 0.508\n",
      "Epoch: 25, Loss: 8.4148, Train: 0.498, Valid: 0.503, Test: 0.506, AUC: 0.508\n",
      "Best epoch: \n",
      "Epoch: 15, Loss: 9.9701, Train: 0.498, Valid: 0.506, Test: 0.504, AUC: 0.508\n",
      "------------------(1/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_10percentDiff\n",
      "Epoch: 0, Loss: 157.2144, Train: 0.533, Valid: 0.528, Test: 0.528, AUC: 0.533\n",
      "Epoch: 5, Loss: 5.5456, Train: 0.499, Valid: 0.503, Test: 0.507, AUC: 0.533\n",
      "Epoch: 10, Loss: 2.3168, Train: 0.651, Valid: 0.653, Test: 0.650, AUC: 0.720\n",
      "Epoch: 15, Loss: 4.7940, Train: 0.569, Valid: 0.579, Test: 0.577, AUC: 0.725\n",
      "Epoch: 20, Loss: 2.8523, Train: 0.671, Valid: 0.671, Test: 0.673, AUC: 0.742\n",
      "Epoch: 25, Loss: 2.7717, Train: 0.661, Valid: 0.663, Test: 0.658, AUC: 0.742\n",
      "Best epoch: \n",
      "Epoch: 17, Loss: 2.7356, Train: 0.675, Valid: 0.675, Test: 0.675, AUC: 0.742\n",
      "------------------(2/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_15percentDiff\n",
      "Epoch: 0, Loss: 52.4841, Train: 0.647, Valid: 0.653, Test: 0.651, AUC: 0.710\n",
      "Epoch: 5, Loss: 6.8744, Train: 0.726, Valid: 0.728, Test: 0.729, AUC: 0.822\n",
      "Epoch: 10, Loss: 1.6808, Train: 0.749, Valid: 0.749, Test: 0.750, AUC: 0.840\n",
      "Epoch: 15, Loss: 13.5459, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.850\n",
      "Epoch: 20, Loss: 1.2164, Train: 0.765, Valid: 0.764, Test: 0.769, AUC: 0.852\n",
      "Epoch: 25, Loss: 2.4282, Train: 0.750, Valid: 0.750, Test: 0.752, AUC: 0.852\n",
      "Best epoch: \n",
      "Epoch: 27, Loss: 1.9424, Train: 0.769, Valid: 0.771, Test: 0.772, AUC: 0.861\n",
      "------------------(3/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_1percentDiff\n",
      "Epoch: 0, Loss: 64.5183, Train: 0.494, Valid: 0.494, Test: 0.487, AUC: 0.488\n",
      "Epoch: 5, Loss: 5.2439, Train: 0.498, Valid: 0.502, Test: 0.507, AUC: 0.484\n",
      "Epoch: 10, Loss: 1.1440, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.484\n",
      "Epoch: 15, Loss: 2.4945, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.484\n",
      "Epoch: 20, Loss: 4.3952, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.484\n",
      "Epoch: 25, Loss: 2.1232, Train: 0.498, Valid: 0.502, Test: 0.506, AUC: 0.484\n",
      "Best epoch: \n",
      "Epoch: 4, Loss: 5.8662, Train: 0.498, Valid: 0.502, Test: 0.506, AUC: 0.484\n",
      "------------------(4/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_2percentDiff\n",
      "Epoch: 0, Loss: 85.4071, Train: 0.519, Valid: 0.525, Test: 0.516, AUC: 0.520\n",
      "Epoch: 5, Loss: 1.0479, Train: 0.499, Valid: 0.503, Test: 0.506, AUC: 0.520\n",
      "Epoch: 10, Loss: 0.9697, Train: 0.499, Valid: 0.503, Test: 0.506, AUC: 0.520\n",
      "Epoch: 15, Loss: 0.9288, Train: 0.498, Valid: 0.503, Test: 0.506, AUC: 0.520\n",
      "Epoch: 20, Loss: 0.8831, Train: 0.507, Valid: 0.503, Test: 0.500, AUC: 0.520\n",
      "Epoch: 25, Loss: 1.5425, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.520\n",
      "Best epoch: \n",
      "Epoch: 0, Loss: 85.4071, Train: 0.519, Valid: 0.525, Test: 0.516, AUC: 0.520\n",
      "------------------(5/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_3percentDiff\n",
      "Epoch: 0, Loss: 245.2870, Train: 0.513, Valid: 0.510, Test: 0.509, AUC: 0.532\n",
      "Epoch: 5, Loss: 4.8205, Train: 0.497, Valid: 0.498, Test: 0.505, AUC: 0.532\n",
      "Epoch: 10, Loss: 7.3826, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.532\n",
      "Epoch: 15, Loss: 6.2085, Train: 0.498, Valid: 0.503, Test: 0.506, AUC: 0.552\n",
      "Epoch: 20, Loss: 7.7882, Train: 0.507, Valid: 0.503, Test: 0.499, AUC: 0.552\n",
      "Epoch: 25, Loss: 3.0758, Train: 0.507, Valid: 0.511, Test: 0.511, AUC: 0.558\n",
      "Best epoch: \n",
      "Epoch: 22, Loss: 6.1115, Train: 0.538, Valid: 0.538, Test: 0.526, AUC: 0.558\n",
      "------------------(6/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_4percentDiff\n",
      "Epoch: 0, Loss: 70.3879, Train: 0.474, Valid: 0.474, Test: 0.477, AUC: 0.466\n",
      "Epoch: 5, Loss: 6.6886, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.564\n",
      "Epoch: 10, Loss: 7.8949, Train: 0.498, Valid: 0.502, Test: 0.506, AUC: 0.574\n",
      "Epoch: 15, Loss: 5.6905, Train: 0.509, Valid: 0.504, Test: 0.502, AUC: 0.574\n",
      "Epoch: 20, Loss: 3.6689, Train: 0.503, Valid: 0.499, Test: 0.495, AUC: 0.574\n",
      "Epoch: 25, Loss: 4.3295, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.574\n",
      "Best epoch: \n",
      "Epoch: 27, Loss: 3.7838, Train: 0.569, Valid: 0.570, Test: 0.566, AUC: 0.605\n",
      "------------------(7/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_5percentDiff\n",
      "Epoch: 0, Loss: 32.3742, Train: 0.541, Valid: 0.548, Test: 0.552, AUC: 0.575\n",
      "Epoch: 5, Loss: 6.6269, Train: 0.502, Valid: 0.498, Test: 0.494, AUC: 0.575\n",
      "Epoch: 10, Loss: 4.1543, Train: 0.545, Valid: 0.543, Test: 0.545, AUC: 0.628\n",
      "Epoch: 15, Loss: 8.7545, Train: 0.519, Valid: 0.516, Test: 0.512, AUC: 0.628\n",
      "Epoch: 20, Loss: 7.2136, Train: 0.502, Valid: 0.498, Test: 0.494, AUC: 0.628\n",
      "Epoch: 25, Loss: 8.5973, Train: 0.559, Valid: 0.559, Test: 0.562, AUC: 0.628\n",
      "Best epoch: \n",
      "Epoch: 9, Loss: 6.4951, Train: 0.582, Valid: 0.589, Test: 0.587, AUC: 0.628\n",
      "------------------(8/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0_7percentDiff\n",
      "Epoch: 0, Loss: 45.2014, Train: 0.422, Valid: 0.424, Test: 0.415, AUC: 0.384\n",
      "Epoch: 5, Loss: 1.3426, Train: 0.605, Valid: 0.605, Test: 0.618, AUC: 0.656\n",
      "Epoch: 10, Loss: 5.2801, Train: 0.589, Valid: 0.590, Test: 0.598, AUC: 0.694\n",
      "Epoch: 15, Loss: 2.5852, Train: 0.534, Valid: 0.534, Test: 0.531, AUC: 0.694\n",
      "Epoch: 20, Loss: 0.8831, Train: 0.548, Valid: 0.549, Test: 0.557, AUC: 0.694\n",
      "Epoch: 25, Loss: 0.9175, Train: 0.637, Valid: 0.640, Test: 0.645, AUC: 0.707\n",
      "Best epoch: \n",
      "Epoch: 25, Loss: 0.9175, Train: 0.637, Valid: 0.640, Test: 0.645, AUC: 0.707\n",
      "------------------(9/9) models trained------------------\n",
      "\n",
      "1553.9916400909424 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "\n",
    "batchSize = 1024\n",
    "print(f'Device: {device}')\n",
    "\n",
    "for i in range(len(datasetDirs)):\n",
    "    dataset = ToyDGLDataset_v2(datasetNames[i], datasetDirs[i])\n",
    "    splitIndices = dataset.get_split_indices()\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(splitIndices['train'])\n",
    "    val_sampler = SubsetRandomSampler(splitIndices['valid'])\n",
    "    test_sampler = SubsetRandomSampler(splitIndices['test'])\n",
    "\n",
    "    train_dataloader = GraphDataLoader(dataset, sampler=train_sampler, batch_size=batchSize, drop_last=False)\n",
    "    val_dataloader = GraphDataLoader(dataset, sampler=val_sampler, batch_size=batchSize, drop_last=False)\n",
    "    test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=batchSize, drop_last=False)\n",
    "    \n",
    "    # Create the model with given dimensions\n",
    "    model = GCN_OnlyNodeFeatures(dataset.dim_nfeats, 16, dataset.num_graph_classes, norm='none').to(device)\n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs = 30\n",
    "    \n",
    "    # train\n",
    "    print(f'Beginning training on dataset {datasetNames[i]}')\n",
    "    results, bestmodel = trainEpochs(model, device, train_dataloader, optimizer, loss_fn, batchSize, epochs)\n",
    "    results.printBestResult()\n",
    "    \n",
    "    # save results\n",
    "    outputFolder = path.join(datasetDirs[i], 'GCN_OnlyNodeFeatures_normNone')\n",
    "    Path(outputFolder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results.savePlots(outputFolder)\n",
    "    results.dumpSummary(outputFolder)\n",
    "    results.pickledump(outputFolder)\n",
    "    \n",
    "    # save the best model for inference. (when loading for inference -> model.eval()!! )\n",
    "    # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n",
    "    torch.save(bestmodel.state_dict(), path.join(outputFolder, 'model.pt'))\n",
    "    \n",
    "    print(f'------------------({i+1}/{len(datasetDirs)}) models trained------------------\\n')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - now\n",
    "print(f'{elapsed} seconds elapsed')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = TrainResults.loadPickle('test.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(graph.nodes[[0,1]])\n",
    "print(graph.nodes[0])\n",
    "print(graph.nodes[1])\n",
    "print(graph.nodes[10])\n",
    "print(graph.nodes[3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#print(graph.out_edges(7))\n",
    "#print(graph.edges[(0,1)]) # src, dst\n",
    "#print(graph.edges[0,1]) # src, dst\n",
    "print(graph.edges[[0, 0], [1,2]]) # src, dst\n",
    "print(graph.edges[[10], [3]])\n",
    "print(graph.edges[[3], [10]])\n",
    "print(graph.edges[0])\n",
    "print(graph.edges[1])\n",
    "#print(graph.edges[0]) # eid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "edgeList = GetNeighborNodes(graph, 0)\n",
    "nodeFeat = graph.ndata['feat']\n",
    "print(graph.nodes[0])\n",
    "print(nodeFeat)\n",
    "print(edgeList)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def ConcatNodeAndEdgeFeatures(graph, nodeFeat, nodeLabel: int):\n",
    "    neighbors = GetNeighborNodes(graph, nodeLabel)\n",
    "    nfeat = nodeFeat[nodeLabel]\n",
    "    efeat = torch.reshape(graph.edges[neighbors].data['feat'], (-1,))\n",
    "    #print(nfeat)\n",
    "    #print(efeat)\n",
    "    return torch.cat((nfeat, efeat))\n",
    "    \n",
    "x = ConcatNodeAndEdgeFeatures(graph, nodeFeat, 0)\n",
    "print(x)\n",
    "\n",
    "edgeFeatureVec = GetEdgeFeatureVectors(graph)\n",
    "\n",
    "x = (1,2,3)\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
