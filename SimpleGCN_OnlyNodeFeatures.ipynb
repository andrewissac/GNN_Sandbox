{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import copy\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from dgl.data import DGLDataset\n",
    "from TrainResults import TrainResults\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from SimpleGCN_OnlyNodeFeatures import GCN_OnlyNodeFeatures\n",
    "from ToyDGLDataset import ToyDGLDataset, GetNodeFeatureVectors, GetEdgeFeatureVectors, GetNeighborNodes, GetEdgeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function\n",
    "@torch.no_grad()\n",
    "def eval(model, device, dataloader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_logits = []\n",
    "\n",
    "    for batched_graph, labels in dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        nodeFeatVec = GetNodeFeatureVectors(batched_graph)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batched_graph, nodeFeatVec)\n",
    "\n",
    "        y_true.append(labels.detach().cpu())\n",
    "        y_logits.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_logits = torch.cat(y_logits, dim = 0)\n",
    "    y_softmax = nn.functional.softmax(y_logits, dim=1)\n",
    "    y_scoreClass1 = y_softmax[:, 1]\n",
    "    y_pred = y_logits.numpy().argmax(1)\n",
    "    \n",
    "    num_correct_pred = (y_pred == y_true).sum().item()\n",
    "    num_total_pred = len(y_true)\n",
    "    acc =  num_correct_pred / num_total_pred\n",
    "    \n",
    "    evalDict = {\n",
    "        \"y_true\": y_true.tolist(), \n",
    "        \"y_logits\": y_logits.tolist(), \n",
    "        \"y_scoreClass1\": y_scoreClass1.tolist(),\n",
    "        \"y_pred\": y_pred.tolist(), \n",
    "        \"acc\": acc\n",
    "    }\n",
    "\n",
    "    return evalDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, dataloader, optimizer, loss_fn, batchsize, results):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    epochLoss = 0.0\n",
    "    batchIter = 0\n",
    "    \n",
    "    for batched_graph, labels in dataloader:\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        nodeFeatVec = GetNodeFeatureVectors(batched_graph)\n",
    "\n",
    "        #forward\n",
    "        pred =  model(batched_graph, nodeFeatVec)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # multiply running loss by the number of graphs, \n",
    "        # since CrossEntropy loss calculates mean of the losses of the graphs in the batch\n",
    "        runningTotalLoss = loss.item() #* batchsize\n",
    "        results.addRunningLoss(runningTotalLoss)\n",
    "        epochLoss += runningTotalLoss\n",
    "        batchIter += 1\n",
    "        \n",
    "    return epochLoss/batchIter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def trainEpochs(model, device, dataloader, optimizer, loss_fn, batchsize, nEpochs):\n",
    "    results = TrainResults()\n",
    "    bestModel = None\n",
    "    bestValAcc = 0.0\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "        # train\n",
    "        epochLoss = train(model, device, dataloader, optimizer, loss_fn, batchsize, results)\n",
    "\n",
    "        # evaluate\n",
    "        train_result = eval(model, device, train_dataloader)\n",
    "        val_result = eval(model, device, val_dataloader)\n",
    "        test_result = eval(model, device, test_dataloader)\n",
    "\n",
    "        results.addEpochResult(epochLoss, train_result, val_result, test_result)\n",
    "\n",
    "        if(epoch % 5 == 0):\n",
    "            results.printLastResult()\n",
    "\n",
    "        if results.best_val_acc > bestValAcc:\n",
    "            bestValAcc = results.best_val_acc\n",
    "            bestModel = copy.deepcopy(model)\n",
    "\n",
    "    return results, bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllDatasetNames(datasetRootDir):\n",
    "    files = glob.glob(datasetRootDir + '/*/*/*.json', recursive=True)\n",
    "    files.sort()\n",
    "    datasetDirectories = [path.dirname(file) for file in files]\n",
    "    datasetnames = [path.normpath(dir).split(path.sep)[-1] for dir in datasetDirectories]\n",
    "    return datasetDirectories, datasetnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "datasetRootDir = '/home/andrew/GNN_Sandbox/GraphToyDatasets_v3'\n",
    "datasetDirs, datasetNames = getAllDatasetNames(datasetRootDir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datasetName = datasetNames[0]\n",
    "datasetDir = datasetDirs[0]\n",
    "dataset = ToyDGLDataset(datasetName, datasetDir)\n",
    "dataset.printProperties()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "graph, label = dataset[0]\n",
    "print(graph)\n",
    "print(f'Label: {label}')\n",
    "print(GetNodeFeatureVectors(graph))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batched_graph, labels = batch\n",
    "print('Number of nodes for each graph element in the batch:', batched_graph.batch_num_nodes())\n",
    "print('Number of edges for each graph element in the batch:', batched_graph.batch_num_edges())\n",
    "print(labels)\n",
    "# Recover the original graph elements from the minibatch\n",
    "graphs = dgl.unbatch(batched_graph)\n",
    "print('The original graphs in the minibatch:')\n",
    "print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_0percentDiff\n",
      "Epoch: 0, Loss: 2.1576, Train: 0.499, Valid: 0.501, Test: 0.496, AUC: 0.501\n",
      "Epoch: 5, Loss: 0.6978, Train: 0.499, Valid: 0.503, Test: 0.506, AUC: 0.501\n",
      "Epoch: 10, Loss: 0.6952, Train: 0.499, Valid: 0.505, Test: 0.500, AUC: 0.501\n",
      "Epoch: 15, Loss: 0.6942, Train: 0.499, Valid: 0.502, Test: 0.503, AUC: 0.501\n",
      "Epoch: 20, Loss: 0.6938, Train: 0.500, Valid: 0.498, Test: 0.505, AUC: 0.501\n",
      "Epoch: 25, Loss: 0.6944, Train: 0.502, Valid: 0.497, Test: 0.494, AUC: 0.501\n",
      "Best epoch: \n",
      "Epoch: 13, Loss: 0.6947, Train: 0.497, Valid: 0.508, Test: 0.499, AUC: 0.501\n",
      "------------------(1/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_10percentDiff\n",
      "Epoch: 0, Loss: 0.8140, Train: 0.436, Valid: 0.443, Test: 0.429, AUC: 0.403\n",
      "Epoch: 5, Loss: 0.6040, Train: 0.700, Valid: 0.695, Test: 0.704, AUC: 0.775\n",
      "Epoch: 10, Loss: 0.5140, Train: 0.781, Valid: 0.775, Test: 0.783, AUC: 0.863\n",
      "Epoch: 15, Loss: 0.4118, Train: 0.852, Valid: 0.847, Test: 0.848, AUC: 0.942\n",
      "Epoch: 20, Loss: 0.3141, Train: 0.898, Valid: 0.898, Test: 0.900, AUC: 0.972\n",
      "Epoch: 25, Loss: 0.2384, Train: 0.934, Valid: 0.932, Test: 0.934, AUC: 0.988\n",
      "Best epoch: \n",
      "Epoch: 28, Loss: 0.2057, Train: 0.957, Valid: 0.958, Test: 0.958, AUC: 0.993\n",
      "------------------(2/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_15percentDiff\n",
      "Epoch: 0, Loss: 0.9105, Train: 0.554, Valid: 0.549, Test: 0.545, AUC: 0.856\n",
      "Epoch: 5, Loss: 0.5154, Train: 0.770, Valid: 0.770, Test: 0.766, AUC: 0.861\n",
      "Epoch: 10, Loss: 0.4047, Train: 0.859, Valid: 0.854, Test: 0.851, AUC: 0.930\n",
      "Epoch: 15, Loss: 0.3023, Train: 0.918, Valid: 0.917, Test: 0.916, AUC: 0.974\n",
      "Epoch: 20, Loss: 0.2110, Train: 0.962, Valid: 0.961, Test: 0.960, AUC: 0.994\n",
      "Epoch: 25, Loss: 0.1437, Train: 0.983, Valid: 0.983, Test: 0.983, AUC: 0.999\n",
      "Best epoch: \n",
      "Epoch: 28, Loss: 0.1156, Train: 0.988, Valid: 0.987, Test: 0.988, AUC: 0.999\n",
      "------------------(3/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_1percentDiff\n",
      "Epoch: 0, Loss: 1.3031, Train: 0.484, Valid: 0.478, Test: 0.487, AUC: 0.486\n",
      "Epoch: 5, Loss: 0.6962, Train: 0.502, Valid: 0.498, Test: 0.493, AUC: 0.492\n",
      "Epoch: 10, Loss: 0.6943, Train: 0.509, Valid: 0.507, Test: 0.502, AUC: 0.523\n",
      "Epoch: 15, Loss: 0.6958, Train: 0.520, Valid: 0.525, Test: 0.516, AUC: 0.534\n",
      "Epoch: 20, Loss: 0.6920, Train: 0.499, Valid: 0.503, Test: 0.507, AUC: 0.541\n",
      "Epoch: 25, Loss: 0.6934, Train: 0.540, Valid: 0.541, Test: 0.532, AUC: 0.552\n",
      "Best epoch: \n",
      "Epoch: 28, Loss: 0.6916, Train: 0.540, Valid: 0.544, Test: 0.538, AUC: 0.555\n",
      "------------------(4/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_2percentDiff\n",
      "Epoch: 0, Loss: 1.5499, Train: 0.476, Valid: 0.479, Test: 0.470, AUC: 0.456\n",
      "Epoch: 5, Loss: 0.6986, Train: 0.498, Valid: 0.502, Test: 0.506, AUC: 0.458\n",
      "Epoch: 10, Loss: 0.6906, Train: 0.527, Valid: 0.521, Test: 0.525, AUC: 0.578\n",
      "Epoch: 15, Loss: 0.6876, Train: 0.552, Valid: 0.544, Test: 0.548, AUC: 0.594\n",
      "Epoch: 20, Loss: 0.6863, Train: 0.569, Valid: 0.562, Test: 0.568, AUC: 0.607\n",
      "Epoch: 25, Loss: 0.6817, Train: 0.580, Valid: 0.574, Test: 0.590, AUC: 0.618\n",
      "Best epoch: \n",
      "Epoch: 25, Loss: 0.6817, Train: 0.580, Valid: 0.574, Test: 0.590, AUC: 0.618\n",
      "------------------(5/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_3percentDiff\n",
      "Epoch: 0, Loss: 0.8403, Train: 0.500, Valid: 0.504, Test: 0.506, AUC: 0.539\n",
      "Epoch: 5, Loss: 0.6877, Train: 0.505, Valid: 0.500, Test: 0.497, AUC: 0.592\n",
      "Epoch: 10, Loss: 0.6810, Train: 0.556, Valid: 0.549, Test: 0.552, AUC: 0.613\n",
      "Epoch: 15, Loss: 0.6775, Train: 0.593, Valid: 0.595, Test: 0.598, AUC: 0.640\n",
      "Epoch: 20, Loss: 0.6666, Train: 0.614, Valid: 0.620, Test: 0.624, AUC: 0.665\n",
      "Epoch: 25, Loss: 0.6614, Train: 0.511, Valid: 0.517, Test: 0.519, AUC: 0.665\n",
      "Best epoch: \n",
      "Epoch: 29, Loss: 0.6496, Train: 0.647, Valid: 0.651, Test: 0.657, AUC: 0.709\n",
      "------------------(6/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_4percentDiff\n",
      "Epoch: 0, Loss: 2.3206, Train: 0.471, Valid: 0.471, Test: 0.462, AUC: 0.436\n",
      "Epoch: 5, Loss: 0.6964, Train: 0.520, Valid: 0.524, Test: 0.520, AUC: 0.532\n",
      "Epoch: 10, Loss: 0.6767, Train: 0.596, Valid: 0.594, Test: 0.602, AUC: 0.643\n",
      "Epoch: 15, Loss: 0.6686, Train: 0.584, Valid: 0.578, Test: 0.583, AUC: 0.659\n",
      "Epoch: 20, Loss: 0.6584, Train: 0.591, Valid: 0.585, Test: 0.589, AUC: 0.680\n",
      "Epoch: 25, Loss: 0.6455, Train: 0.653, Valid: 0.648, Test: 0.654, AUC: 0.725\n",
      "Best epoch: \n",
      "Epoch: 29, Loss: 0.6326, Train: 0.679, Valid: 0.676, Test: 0.679, AUC: 0.753\n",
      "------------------(7/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_5percentDiff\n",
      "Epoch: 0, Loss: 1.4819, Train: 0.544, Valid: 0.545, Test: 0.544, AUC: 0.558\n",
      "Epoch: 5, Loss: 0.6736, Train: 0.589, Valid: 0.588, Test: 0.595, AUC: 0.629\n",
      "Epoch: 10, Loss: 0.6604, Train: 0.606, Valid: 0.605, Test: 0.605, AUC: 0.668\n",
      "Epoch: 15, Loss: 0.6449, Train: 0.609, Valid: 0.610, Test: 0.621, AUC: 0.706\n",
      "Epoch: 20, Loss: 0.6224, Train: 0.685, Valid: 0.687, Test: 0.687, AUC: 0.767\n",
      "Epoch: 25, Loss: 0.5996, Train: 0.687, Valid: 0.685, Test: 0.685, AUC: 0.798\n",
      "Best epoch: \n",
      "Epoch: 29, Loss: 0.5779, Train: 0.750, Valid: 0.752, Test: 0.755, AUC: 0.838\n",
      "------------------(8/9) models trained------------------\n",
      "\n",
      "Done loading data from cached files.\n",
      "Beginning training on dataset Toy2_v3_7percentDiff\n",
      "Epoch: 0, Loss: 1.9522, Train: 0.487, Valid: 0.491, Test: 0.491, AUC: 0.464\n",
      "Epoch: 5, Loss: 0.6666, Train: 0.619, Valid: 0.620, Test: 0.619, AUC: 0.668\n",
      "Epoch: 10, Loss: 0.6386, Train: 0.651, Valid: 0.652, Test: 0.653, AUC: 0.711\n",
      "Epoch: 15, Loss: 0.6139, Train: 0.690, Valid: 0.685, Test: 0.693, AUC: 0.761\n",
      "Epoch: 20, Loss: 0.5798, Train: 0.730, Valid: 0.726, Test: 0.731, AUC: 0.809\n",
      "Epoch: 25, Loss: 0.5420, Train: 0.763, Valid: 0.760, Test: 0.762, AUC: 0.847\n",
      "Best epoch: \n",
      "Epoch: 29, Loss: 0.5071, Train: 0.811, Valid: 0.810, Test: 0.810, AUC: 0.894\n",
      "------------------(9/9) models trained------------------\n",
      "\n",
      "1819.7657725811005 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time()\n",
    "\n",
    "batchSize = 1024\n",
    "print(f'Device: {device}')\n",
    "\n",
    "for i in range(len(datasetDirs)):\n",
    "    dataset = ToyDGLDataset(datasetNames[i], datasetDirs[i])\n",
    "    splitIndices = dataset.get_split_indices()\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(splitIndices['train'])\n",
    "    val_sampler = SubsetRandomSampler(splitIndices['valid'])\n",
    "    test_sampler = SubsetRandomSampler(splitIndices['test'])\n",
    "\n",
    "    train_dataloader = GraphDataLoader(dataset, sampler=train_sampler, batch_size=batchSize, drop_last=False)\n",
    "    val_dataloader = GraphDataLoader(dataset, sampler=val_sampler, batch_size=batchSize, drop_last=False)\n",
    "    test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=batchSize, drop_last=False)\n",
    "    \n",
    "    # Create the model with given dimensions\n",
    "    model = GCN_OnlyNodeFeatures(dataset.dim_nfeats, 16, dataset.num_graph_classes).to(device)\n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs = 30\n",
    "    \n",
    "    # train\n",
    "    print(f'Beginning training on dataset {datasetNames[i]}')\n",
    "    results, bestmodel = trainEpochs(model, device, train_dataloader, optimizer, loss_fn, batchSize, epochs)\n",
    "    results.printBestResult()\n",
    "    \n",
    "    # save results\n",
    "    outputFolder = path.join(datasetDirs[i], 'GCN_OnlyNodeFeatures_lossPerTrainingstep__GPU')\n",
    "    Path(outputFolder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results.savePlots(outputFolder)\n",
    "    results.dumpSummary(outputFolder)\n",
    "    results.pickledump(outputFolder)\n",
    "    \n",
    "    # save the best model for inference. (when loading for inference -> model.eval()!! )\n",
    "    # https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n",
    "    torch.save(bestmodel.state_dict(), path.join(outputFolder, 'model.pt'))\n",
    "    \n",
    "    print(f'------------------({i+1}/{len(datasetDirs)}) models trained------------------\\n')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - now\n",
    "print(f'{elapsed} seconds elapsed')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = TrainResults.loadPickle('test.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(graph.nodes[[0,1]])\n",
    "print(graph.nodes[0])\n",
    "print(graph.nodes[1])\n",
    "print(graph.nodes[10])\n",
    "print(graph.nodes[3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#print(graph.out_edges(7))\n",
    "#print(graph.edges[(0,1)]) # src, dst\n",
    "#print(graph.edges[0,1]) # src, dst\n",
    "print(graph.edges[[0, 0], [1,2]]) # src, dst\n",
    "print(graph.edges[[10], [3]])\n",
    "print(graph.edges[[3], [10]])\n",
    "print(graph.edges[0])\n",
    "print(graph.edges[1])\n",
    "#print(graph.edges[0]) # eid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "edgeList = GetNeighborNodes(graph, 0)\n",
    "nodeFeat = graph.ndata['feat']\n",
    "print(graph.nodes[0])\n",
    "print(nodeFeat)\n",
    "print(edgeList)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def ConcatNodeAndEdgeFeatures(graph, nodeFeat, nodeLabel: int):\n",
    "    neighbors = GetNeighborNodes(graph, nodeLabel)\n",
    "    nfeat = nodeFeat[nodeLabel]\n",
    "    efeat = torch.reshape(graph.edges[neighbors].data['feat'], (-1,))\n",
    "    #print(nfeat)\n",
    "    #print(efeat)\n",
    "    return torch.cat((nfeat, efeat))\n",
    "    \n",
    "x = ConcatNodeAndEdgeFeatures(graph, nodeFeat, 0)\n",
    "print(x)\n",
    "\n",
    "edgeFeatureVec = GetEdgeFeatureVectors(graph)\n",
    "\n",
    "x = (1,2,3)\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
